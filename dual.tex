\subsection{Duality}
\label{sec:duality}

In applied linear algebra, we distinguish between column and row vectors; in pure linear algebra, vector spaces and dual vector spaces; in tensor algebra, contravariant and covariant indices; in quantum mechanics, bras and kets. Do we need something like this?

In \S\ref{sec:rnn} we saw that defining an RNN requires renaming of axes, because a linear transformation must map one axis to another axis; if we want to map an axis to itself, we need to use renaming.

In this section, we describe three possible solutions to this problem, and welcome comments about which (if any) would be best.

\subsubsection{Contracting two names}

We define a version of the contraction operator that can contract two axes with different names (and the same size):
\begin{align*}
\nndot{ax1}{ax2} &: F^{\nset{ax1}{n}} \times F^{\nset{ax2}{n}} \rightarrow F \\
A \nndot{ax1}{ax2} B &= \sum_{i=1}^n A_{\nidx{ax1}{i}} B_{\nidx{ax2}{i}}.
\end{align*}

For example, the RNN would look like this.
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden'}} & |\name{hidden}| &= |\name{hidden'}| \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \name{hidden}} \\
b &\in \mathbb{R}^{\name{hidden}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \sigma\left( W^{\text{h}} \nndot{hidden'}{hidden} h^{t-1} + W^{\text{i}} \ndot{input} x^{t} + b \right) & t &= 1, \ldots, n
\end{align*}

In self-attention, it seems unavoidable to have one renaming (corresponding to the transpose in the original formula):
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \nfun{seq'}{softmax} \left( \frac{Q \ndot{key} \nmov{seq}{seq'}{K}}{\sqrt{|\name{key}|}} \right) \nndot{seq'}{seq} V \\
  |\name{seq}| &= |\name{seq'}| \\
  Q &= W^{l,Q} \ndot{layer} X & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  Y &= \nsum{heads} W^{l,O} \ndot{val} \text{Att}(Q, K, V, M) & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

Multivariate normal:
\begin{align*}
\mathcal{N} \colon \reals^{\name{d}} &\rightarrow \reals \\
\mathcal{N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\frac{1}{2}  \left(\nfun{d1, d2}{inv}(\Sigma) \nndot{d1}{d} (X - \mu) \right) \nndot{d2}{d} (X - \mu) \right)}{\sqrt{(2 \pi)^{|\name{d}|} \nfun{d1, d2}{det}(\Sigma)}}
\end{align*}
where
\begin{align*}
|\name{d}| &= |\name{d1}| = |\name{d2}| \\
\mu &\in \reals^{\name{d}} \\
\Sigma & \in \reals^{\name{d1} \times \name{d2}}.
\end{align*}

\subsubsection{Starred axis names}

If $\name{ax}$ is a name, we also allow a tensor to have an axis $\name{ax*}$ with the same index set. (Alternatively, we could write one as a superscript and one as a subscript, as is done with contravariant and covariant indices.) Multiplication contracts starred axes in the left operand with non-starred axes in the right operand.
There are a few variants of this idea that have been floated:
\begin{enumerate}
\item $\ndot{}$ (no subscript) contracts every starred axis in its left operand with every corresponding unstarred axis in its right operand.
\item $\ndot{ax}$ contracts $\name{ax}$ with $\name{ax}$, and we need another notation like $\ndot{ax(*)}$ or $\nbin{ax}{\times}$ for contracting $\name{ax*}$ with $\name{ax}$.
\item $\ndot{ax}$ always contracts $\name{ax*}$ with $\name{ax}$; there's no way to contract $\name{ax}$ with $\name{ax}$.
\end{enumerate}

RNN:
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden*}} \\
W^{\text{i}} &\in \mathbb{R}^{\name{hidden} \times \name{input*}} \\
b &\in \mathbb{R}^{\name{hidden}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \sigma\left( W^{\text{h}} \ndot{hidden} h^{t-1} + W^{\text{i}} \ndot{input} x^{t} + b \right) & t &= 1, \ldots, n
\end{align*}

Self-attention: Again one renaming seems unavoidable.
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \nfun{seq*}{softmax} \left( \frac{Q \ndot{key} \nmov{seq}{seq*}{K}}{\sqrt{|\name{key}|}} \right) \ndot{seq} V \\
  Q &= W^{l,Q} \ndot{layer} X & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  Y &= \nsum{heads} W^{l,O} \ndot{val} \text{Att}(Q, K, V, M) & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

Starred axes don't really help much with the multivariant normal distribution.

\subsubsection{Named and numbered axes}
\label{sec:tensorsoftensors}

We allow axes to have names that are natural numbers $1, 2, \ldots$, and we define ``numbering'' and ``naming'' operators:
\begin{center}
\begin{tabular}{cl}
$A_{\name{ax}}$ & rename axis $\name{ax}$ to 1 \\
$A_{\name{ax1},\name{ax2}}$ & rename axis $\name{ax1}$ to 1 and $\name{ax2}$ to 2 \\
$A_{\rightarrow\name{ax}}$ & rename axis 1 to $\name{ax}$ \\
$A_{\rightarrow\name{ax1},\name{ax2}}$ & rename axis 1 to $\name{ax1}$ and 2 to $\name{ax2}$
\end{tabular}
\end{center}
The numbering operators are only defined on tensors that have no numbered axes.

Then we adopt the convention that standard vector/matrix operations operate on the numbered axes. For example, vector dot-product always uses axis 1 of both its operands, so that we can write
\begin{equation*}
C = A_{\name{ax}} \cdot B_{\name{ax}}
\end{equation*}
equivalent to $C = A \ndot{ax} B$. 

Previously, we had to define a new version of every operation; most of the time, it looked similar to the standard version (e.g., $\max$ vs $\max_{\name{ax}}$), but occasionally it looked quite different (e.g., matrix inversion). With numbered axes, we can use standard notation for everything.
(This also suggests a clean way to integrate code that uses named tensors with code that uses ordinary tensors.)

We also get the renaming operation for free: $A_{\name{ax1}\rightarrow\name{ax2}} = [A_{\name{ax1}}]_{\rightarrow\name{ax2}}$ renames axis $\name{ax1}$ to $\name{ax2}$.

Finally, this notation alleviates the duality problem, as can be seen in the definition of a RNN:
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden'}} & |\name{hidden}| &= |\name{hidden'}| \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \name{hidden}} \\
b &\in \mathbb{R}^{\name{hidden}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \sigma\left( W^{\text{h}}_{\name{hidden'}} \cdot h^{t-1}_{\name{hidden}} + W^{\text{i}}_{\name{input}} \cdot x^{t}_{\name{input}} + b_{\name{hidden}} \right) & t &= 1, \ldots, n
\end{align*}

Attention requires either a renaming or a transpose:
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \softmax \left[ \frac{Q_{\name{key}} \cdot \nmov{seq}{seq'}{K_{\name{key}}}}{\sqrt{|\name{key}|}} \right]_{\name{seq'}} \cdot V_{\name{seq}} \\
  |\name{seq}| &= |\name{seq'}| \\
  Q &= W^{l,Q} \ndot{layer} X & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  Y &= \nsum{heads} W^{l,O} \ndot{val} \text{Att}(Q, K, V, M) & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

The multivariate normal distribution also needs a renaming or transpose:
\begin{align*} 
X &\in \mathbb{R}^{\name{d}}  \\
\mu &\in \mathbb{R}^{\name{d}}  \\
\Sigma & \in \mathbb{R}^{\name{d1} \times \name{d2}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\tfrac{1}{2} [X - \mu]_{\name{d}}^\top \, \Sigma_{\name{d1},\name{d2}}^{-1} \, [X - \mu]_{\name{d}} \right)}{\sqrt{(2 \pi)^k \, \mathop{\text{det}} \Sigma_{\name{d1},\name{d2}}}}
\end{align*}

Because this notation can be a little more verbose (often requiring you to write axis names twice), we'd keep around the notation $A \ndot{ax} B$ as a shorthand for $A_{\name{ax}} \cdot B_{\name{ax}}$.

