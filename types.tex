\subsection{Index types}

We have defined an axis as a pair $\nset{ax}{I}$, where $\name{ax}$ is a name and $I$ is a set, usually $[n]$ for some $n$. In this section, we consider some other possibilities for~$I$.

\subsubsection{Non-integral types}

The sets $I$ don't have to contain integers. For example, if $V$ is the vocabulary of a natural language ($V = \{ \name{cat}, \name{dog}, \ldots \}$), we could define a matrix of word embeddings:
\begin{align*}
  E &\in \mathbb{R}^{\nset{vocab}{V} \times \nset{emb}{d}}.
\end{align*}

\subsubsection{Integers with units}

If $\name{u}$ is a symbol and $n > 0$, define $[n]\name{u} = \{1\name{u}, 2\name{u}, \ldots, n\name{u}\}$. You could think of $\name{u}$ as analogous to a physical unit, like kilograms. The elements of $[n]\name{u}$ can be added and subtracted like integers ($a\name{u} + b\name{u} = (a+b)\name{u}$) or multiplied by unitless integers ($c \cdot a\name{u} = (c \cdot a) \name{u}$), but numbers with different units are different ($a \name{u} \neq a \name{v}$).

Then the set $[n]\name{u}$ could be used as an index set, which would prevent the axis from being aligned with another axis that uses different units. For example, if we want to define a tensor representing an image, we might write
\[ A \in \mathbb{R}^{\nset{height}{[h]\name{pixels}} \times \nset{width}{[w]\name{pixels}}}. \]
If we have another tensor representing a go board, we might write
\[ B \in \mathbb{R}^{\nset{height}{[n]\name{points}} \times \nset{width}{[n]\name{points}}}, \]
and even if it happens that $h = w = n$, it would be incorrect to write $A+B$ because the units do not match.

\subsubsection{Tuples of integers}

An index set could also be $[m] \times [n]$, which would be a way of sneaking ordered indices into named tensors, useful for matrix operations. For example, the RNN would look like this:
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\nset{hidden}{d \times d}} \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \nset{hidden}{d}} \\
b &\in \mathbb{R}^{\nset{hidden}{d}} \\
h^{0} &\in \mathbb{R}^{\nset{hidden}{d}} \\
h^{t} &= \sigma\left( W^{\text{h}} \nbin{hidden}{\circ} h^{t-1} + W^{\text{i}} \ndot{input} x^{t} + b \right) & t &= 1, \ldots, n
\end{align*}
where $\circ$ does matrix-matrix and matrix-vector multiplication.

This wouldn't really help with self-attention. The multivariate normal distribution would need a transpose.
