\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{parskip}

\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}[0]{\mathbb{R}}

% Named tensor prelude
\newcommand{\nsum}[1]{\mathbin{\mathop{\sum}\displaylimits_{\name{#1}}}}
\newcommand{\nfun}[2]{\underset{\name{#1}}{#2}}

\newcommand{\name}[1]{\mathsf{#1}}
\newcommand{\ndot}[1]{\mathbin{\mathop{\thinspace\boldsymbol\cdot\thinspace}\displaylimits_{\name{#1}}}}

\begin{document}


\subsection*{Attention}

Standard (Single Headed):

\begin{align*} 
\text{attention}(Q, K, V) &=  \softmax \left( \frac{Q K^\top }{\sqrt{d_k}} \right)  V 
\end{align*}


Named Tensor: 

\begin{align*} 
Q &\in \reals^{\name{head}:h, \name{key}{d_v},\name{time'}:n}\\
K &\in \reals^{\name{head}:h,\name{key}:d_k, \name{time}{n}}\\
V &\in \reals^{\name{head}:h, \name{val}:d_v, \name{time}:n}\\
\text{attention}(Q, K, V) &=  \nfun{time}{\softmax} \left( \frac{Q \ndot{key} K }{\sqrt{d_k}} \right) \ndot{time} V 
\end{align*}



\subsection*{MLP}
% Standard (unbatched):

% \begin{align*} 
% V &\in \reals^{o \times  h},\ c\in \reals^{o} \\
% W &\in \reals^{h \times i}, \ b \in \reals^{h} \\
% X &\in \reals^{i} \\
% \text{MLP}(x; V, W,b, c) &= \left( V \sigma \left( W x + b \right) + c \right)  
% \end{align*}

Named Tensor:

\begin{align*} 
V &\in \reals^{\name{output}:o, \name{hidden}:h},\ c\in \reals^{\name{output}:o} \\
W &\in \reals^{{\name{hidden}:h, \name{in}:i}}, \ b \in \reals^{\name{hidden}:h} \\
X &\in \reals^{{\name{batch}:b, \name{in}:i}} \\
\text{MLP}(X; V, W,b, c) &= \sigma \left( V \ndot{hidden} \sigma \left( W \ndot{in} X + b \right) + c \right)  
\end{align*}

\subsection*{NN ``Norms''}

% Standard:  These are often informally described with the same equation, but correspond to very functions.

% \begin{align*} 
% \text{X-norm}(X; \gamma, \beta) &= \frac{X - \text{mean}[X]}{\sqrt{\text{var}(X)} + \epsilon } \odot \gamma + \beta
% \end{align*}


\subsubsection*{Batch Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}}\\
\gamma, \beta &\in \reals^{{\name{batch}:b}} \\
\text{batchnorm}(X; \gamma, \beta) &= \frac{X - \nfun{batch}{\text{mean}}(X)}{\sqrt{\nfun{batch}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}


\subsubsection*{Instance Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}}\\
\gamma, \beta &\in \reals^{{\name{hidden}:h}} \\
\text{instancenorm}(X; \gamma, \beta) &= \frac{X - \nfun{hidden}{\text{mean}}[X]}{\sqrt{\nfun{hidden}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}

\subsubsection*{Layer Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}} \\
\gamma, \beta &\in \reals^{{\name{channel}:c, \name{hidden}:h}} \\
\text{layernorm}(X; \gamma, \beta) &= \frac{X - \nfun{hidden,channel}{\text{mean}}[X]}{\sqrt{\nfun{hidden, channel}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta 
\end{align*}


\subsection*{Continuous Bag of Words}

A continutous bag-of-words model classifies by summing up the embeddings of a sequence of words and then projecting them to the space of classes. 

\begin{align*} 
X &\in \{0, 1\}^{{\name{time}:t, \name{vocab}:v}}\ s.t. \ \nsum{v} X = 1\\
E &\in \reals^{{\name{vocab}:v, \name{hidden}:h}}\\
W &\in \reals^{{\name{class}:c, \name{hidden}:h}}\\
\text{cbow}(X; E, W) &= \nfun{class}{\text{softmax}} (W \ndot{hidden} E \ndot{vocab} X)
\end{align*}

\section*{Bayes' Rule}

Given $p(B \mid A)$ and $p(A)$ as tensors. Bayes' rule computes $p(A \mid B)$ by applying the chain rule, computing the marginal, and broadcast dividing. 

\begin{align*} 
BA &\in [0, 1]^{\name{A}{a}, \name{B}{b}}, 
A \in [0, 1]^{\name{A}{a}} \ s.t. \nsum{A} A =  \nsum{B} BA =  1 \\
AB &= (BA \ndot{} A)  / (BA \ndot{A} A)
\end{align*}


\subsection*{Beam Search}


\begin{align*} 
H &\in \reals^{{ \name{batch}:b, \name{beam}:k}} \\
S &\in \{0, 1\}^{{ \name{batch}:b, \name{beam}:k, \name{state}:s}} \ s.t. \ \nsum{state} X = 1\\
f &\in \{0, 1\}^{{\name{state}}} \mapsto \reals^{{\name{state'}}} \\ 
\text{beamstep}(H, S) &=  \nfun{beam, state'}{\text{max-k}} \left( \nfun{state'}{\text{softmax}}(f(S)) \odot H  \right)
\end{align*} 

Beam search is a commonly used approach for approximate discrete search. Here $H$ is the score of each element in the beam, $S$ is the state of each element in the beam, and $f$ is an update function that returns the score of each state transition. 
Beam step returns the new $H$ tensor. 

\subsection*{Sudoku ILP}

Constraint set to ensure a valid Sudoku puzzle. Sudoku is a tensor binary tiled tensor. With Row / Col subgrids of size row / col. Each cell is has assign possible assignments. 
Constraints need to ensure that there is one digit per row, per column and per sub-box. 

\begin{align*} 
X &\in \{0, 1\}^{\name{row}: 9, \name{col}:9, \name{assign}:9}  \\
\nsum{assign} Y &= 
\nsum{Row,row} Y = 
\nsum{Col, col} Y =  
\nsum{row, col} Y = 1\\
Y &\in \{0, 1\}^{\name{Row}: 3, \name{Col}:3, \name{row}: 3, \name{col}:3, \name{assign}:9}  \\
Y_{\name{Row}:r, \name{row}:r', \name{Col}:c, \name{col}:c'} &= X_{\name{row}:r*3 + r'-1, \name{cow}:c*3 + c'-1}, 
\end{align*} 


\subsection*{Image max pooling}

Max pooling tasks a similar form as the Sudoku example.

\begin{align*} 
X &\in \reals^{\name{height}: h, \name{width}:w} \\
\text{maxpool2d}(X, kh, kw) &=  \nfun{kh, kw}{\max}\  U \\
U &\in \reals^{{\name{height}:h / kh,
\name{width}:w / kw, \name{kh}:kh, \name{kw}:kw}}  \\
U_{\name{height}:i, \name{width}:j, \name{kh}:di, \name{kw}:dj  } & = X_{\name{height}:i * kh + di -1, \name{width}:j * kw + dj -1}  
\end{align*}



\subsection*{Explicit 1D Conv}

\begin{align*} 
X &\in \reals^{{\name{channel}:c, \name{time}:t}}  \\
W &\in \reals^{{\name{out\_channel}:c', \name{channel}:c, \name{kernel}:k}}  \\
\text{conv1d}(X, W) &= W \ndot{channel, kernel} U \\
U &\in \reals^{{\name{channel}:c, \name{time}:t-k +1, \name{kernel}:k}}  \\
U_{\name{time}:i, \name{kernel:j}} & = X_{\name{time}:i+j - 1}  
\end{align*} 


\subsection*{K-Means Clustering}

\begin{align*} 
X &\in \reals^{\name{batch}:b, \name{dim}:k}  \\
C &\in \reals^{\name{cluster}:c, \name{dim}:k}  \\
\text{kmeans}(X, C) &= \nsum{batch} \frac{ Q \odot X}{  Q} \\
% Q &\in \{0, 1\}^{\name{batch}{b},\name{cluster}{c}} \\
Q &= \nfun{cluster}{\arg\min} \ \nfun{dim}{\text{norm}}(C-X
)\end{align*}
Runs one step of kmeans clustering. Bottom term computes cluster alignments $Q$ top term recomputes cluster centers $C$. Like softmax, argmin keeps the same dimensions but uses a 1-hot encoding.

\end{document}

\subsection*{Multivariate Normal}

Bilinear terms are an area that is more verbose with the named tensor notation. However it is also more explicit.

\begin{align*} 
X &\in \reals^{\name{batch}{b}, \name{d}{k}}  \\
\mu &\in \reals^{{\name{d}{k}}}  \\
\Sigma & \in   \reals^{{\name{d1}{k}, \name{d2}{k}}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\frac{1}{2}  \left(\nfun{d1, d2}{\text{inv}}(\Sigma) \ndot{d1} [X - \mu]_{\name{d} \rightarrow \name{d1}} \right) \ndot{d2} [X - \mu]_{\name{d} \rightarrow \name{d2}} \right) }{\sqrt{(2 \pi)^k \nfun{d1, d2}{\text{det}}(\Sigma)} }
\end{align*}


\subsection*{Attention with Causal Masking (For Generation)}



\begin{align*} 
Q &\in \reals^{\name{key}:d_v,\name{time'}:n}\\
K &\in \reals^{\name{head}:h,\name{key}:d_k, \name{time}:n}\\
V &\in \reals^{\name{head}:h, \name{val}:d_v, \name{time}:n}\\
\text{attention}(Q, K, V) &=  \nfun{time}{\softmax} \left( \frac{Q \ndot{key} K }{\sqrt{d_k}} + M \right) \ndot{time} V \\
M & \in \reals^{\name{time}:n, \name{time'}:n} \\
M_{\name{time}:i, \name{time'}:j} & = \begin{cases}0 &\text{\ if\ } i \leq j\\
-\infty & \text{o.w} \end{cases} \\
\end{align*}





\subsection*{Elman RNN}

This is one way to do it that make the recursion explicit. 

\begin{align*} 
X &\in \{0, 1\}^{{\name{time}{t}, \name{vocab}{v}}} \ s.t. \ \nsum{v} X = 1\\
W &\in \reals^{{\name{hidden'}:h, \name{hidden}:h}}\\
E &\in \reals^{{\name{vocab}:v, \name{hidden'}:h}}\\
\text{RNN}(X; E, W) &= \text{step}(X_{\name{time}:t}, \text{step}(X_{\name{time}:t-1}, \text{step}(...) ) \\ 
\text{step}(X, H) &= \left[ \sigma (W \ndot{hidden} H +   E \ndot{vocab} X_{\name{time}:1} ) \right]_{\name{hidden'} \rightarrow \name{hidden}} \\
\end{align*}



\subsection*{Performer Attention Decomposition}


Matmul: 

Recent paper on Performer Attention uses the following notation. 


\begin{align*} 
Q &\in \reals^{{ n \times d_v }}\\
K &\in \reals^{{ n \times d_v }}\\
V &\in \reals^{{ n \times d_v }}\\
\text{attention}(Q, K, V) &=  D^{-1} A V \\
A & = \exp\left( \frac{Q K^\top }{\sqrt{d_k}} \right) \\
D & = \text{diag}(A 1_L) 
\end{align*} 

Named Tensor: 

\begin{align*} 
Q &\in \reals^{{ \name{key}:d_v, \name{time'}:n }}\\
K &\in \reals^{{ \name{key}:d_v, \name{time}:n}}\\
V &\in \reals^{{ \name{val}:d_v, \name{time}:n}}\\
\text{attention}(Q, K, V) &=  \frac{ A \ndot{time} V }{ \nsum{time} A }  \\
A & = \exp  \frac{Q \ndot{key} K }{\sqrt{d_k}}  \\
\end{align*} 

